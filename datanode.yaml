# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Deleting a daemonset may need some trick. See
# https://github.com/kubernetes/kubernetes/issues/33245#issuecomment-261250489
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: hdfs-datanode
spec:
  template:
    metadata:
      labels:
        name: hdfs-datanode
    spec:
      hostNetwork: true
      hostPID: true
      dnsPolicy: ClusterFirstWithHostNet
      containers:
        - name: datanode
          image: uhopper/hadoop-datanode:2.7.2
          env:
            # The following env vars are listed according to low-to-high precedence order.
            # i.e. Whoever comes last will override the earlier value of the same variable.
            - name: CORE_CONF_fs_defaultFS
              value: hdfs://hdfs-namenode-0.hdfs-namenode.default.svc.cluster.local:8020
            - name: MULTIHOMED_NETWORK
              value: "0"
            # The below uses two loops to make sure the last item does not have comma. It uses index 0
            # for the last item since that is the only special index that helm template gives us.
            - name: HDFS_CONF_dfs_datanode_data_dir
              value: |-
                  /hadoop/dfs/data/0
          livenessProbe:
            initialDelaySeconds: 30
            httpGet:
              host: 127.0.0.1
              path: /
              port: 50075
          securityContext:
            privileged: true
          volumeMounts:
            - name: hdfs-data-0
              mountPath: /hadoop/dfs/data/0
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                - key: hdfs-datanode-exclude
                  operator: DoesNotExist
      restartPolicy: Always
      volumes:
        - name: hdfs-data-0
          hostPath:
            path: /hdfs-data
